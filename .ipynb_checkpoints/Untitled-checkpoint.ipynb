{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "import gym_aero as ga\n",
    "import scipy\n",
    "from torch.distributions import Normal\n",
    "from collections import namedtuple\n",
    "import utilities as utils\n",
    "from math import log\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class IndependentGaussianPolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    Gaussian policy function for continuous control tasks. Assumes all actions are\n",
    "    independent (i.e. diagonal covariance matrix).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(IndependentGaussianPolicy, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.mu = nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
    "                               nn.Tanh(),\n",
    "                               nn.Linear(hidden_dim, hidden_dim),\n",
    "                               nn.Tanh(),\n",
    "                               nn.Linear(hidden_dim, output_dim))     \n",
    "        self.logsigma = nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
    "                               nn.Tanh(),\n",
    "                               nn.Linear(hidden_dim, hidden_dim),\n",
    "                               nn.Tanh(),\n",
    "                               nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mu = self.mu(x)\n",
    "        logsigma = self.logsigma(x)\n",
    "        return mu, logsigma\n",
    "\n",
    "class ValueNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple parameterized value function. We use this for both state value functions,\n",
    "    and state-action value functions.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.value = nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
    "                               nn.Tanh(),\n",
    "                               nn.Linear(hidden_dim, hidden_dim),\n",
    "                               nn.Tanh(),\n",
    "                               nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.value(x)\n",
    "        return value\n",
    "    \n",
    "class REINFORCE(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Monte Carlo Policy Gradient. This implementation is single-threaded only, and uses\n",
    "    the score function estimator to find the policy gradient. We use importance sampling to\n",
    "    take multiple update steps at the end of each trajectory. This is necessary to correct\n",
    "    for the fact that the value function was estimated under a previous policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, beta, v_fn):\n",
    "        super(REINFORCE, self).__init__()\n",
    "        self.beta = beta\n",
    "        self.v_fn = v_fn\n",
    "        \n",
    "    def select_action(self, x):\n",
    "        mu, logsigma = self.beta(x)\n",
    "        sigma = torch.exp(logsigma)\n",
    "        dist = Normal(mu, sigma)\n",
    "        action = dist.sample()\n",
    "        lp = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        return action, lp, entropy\n",
    "\n",
    "    def get_phi(self, trajectory, critic, gamma=0.99, tau=0):\n",
    "        states = torch.stack(trajectory[\"states\"]).to(device)\n",
    "        rewards = torch.stack(trajectory[\"rewards\"]).to(device)\n",
    "        next_states = torch.stack(trajectory[\"next_states\"]).to(device)\n",
    "        masks = torch.stack(trajectory[\"masks\"]).to(device)\n",
    "\n",
    "        values = critic(states)\n",
    "        returns = torch.Tensor(rewards.size(0),1).to(device)\n",
    "        deltas = torch.Tensor(rewards.size(0),1).to(device)\n",
    "        advantages = torch.Tensor(rewards.size(0),1).to(device)\n",
    "        prev_return = 0\n",
    "        prev_value = 0\n",
    "        prev_advantage = 0\n",
    "        for i in reversed(range(rewards.size(0))):\n",
    "            returns[i] = rewards[i] + gamma * prev_return * masks[i]\n",
    "            deltas[i] = rewards[i] + gamma * prev_value * masks[i] - values.data[i]\n",
    "            advantages[i] = deltas[i] + gamma * tau * prev_advantage * masks[i]\n",
    "            prev_return = returns[i, 0]\n",
    "            prev_value = values.data[i, 0]\n",
    "            prev_advantage = advantages[i, 0]\n",
    "        return deltas, returns\n",
    "\n",
    "    def update(self, optim, trajectory, iters=4):\n",
    "        log_probs = torch.stack(trajectory[\"log_probs\"]).to(device)\n",
    "        states = torch.stack(trajectory[\"states\"]).to(device)\n",
    "        actions = torch.stack(trajectory[\"actions\"]).to(device)\n",
    "        for i in range(iters):\n",
    "            deltas, returns = self.get_phi(trajectory, self.v_fn)\n",
    "            phi = deltas / returns.std()\n",
    "            v_fn_loss = torch.mean((self.v_fn(states) - returns.detach()) ** 2)\n",
    "            mu_p, logsigma_p = self.beta(states)\n",
    "            sigma_p = torch.exp(logsigma_p)\n",
    "            dist_p = Normal(mu_p, sigma_p)\n",
    "            lp_p = dist_p.log_prob(actions)\n",
    "            ratio = torch.exp((lp_p - log_probs.detach()).sum(dim=-1, keepdim=True))\n",
    "            pol_loss = -torch.mean(ratio * phi.detach())\n",
    "            loss = pol_loss + v_fn_loss\n",
    "            optim.zero_grad()\n",
    "            if i < iters-1:\n",
    "                loss.backward(retain_graph=True)\n",
    "            else:\n",
    "                loss.backward()\n",
    "            optim.step()\n",
    "        return pol_loss.item(), v_fn_loss.item()\n",
    "\n",
    "        \n",
    "class PPO(REINFORCE):\n",
    "    \"\"\"\n",
    "    Simple implementation of the proximal policy optimization algorithm (Schulman, 2017). The\n",
    "    only difference between this and a standard REINFORCE algorithm is the use of the clipped\n",
    "    objective, which helps keep the policy updates bound to a trust region (though there is some\n",
    "    controversy around this).\n",
    "\n",
    "    This implementation uses the same importance sampling correction as in the REINFORCE implementation\n",
    "    above, but uses a clipped surrogate objective to keep the policy update bounded to a trust \n",
    "    region.\n",
    "    \"\"\"\n",
    "    def __init__(self, beta, v_fn):\n",
    "        super(PPO, self).__init__(beta, v_fn)\n",
    "    \n",
    "    def update(self, optim, trajectory, iters=4, eps=0.2):\n",
    "        log_probs = torch.stack(trajectory[\"log_probs\"]).to(device)\n",
    "        states = torch.stack(trajectory[\"states\"]).to(device)\n",
    "        actions = torch.stack(trajectory[\"actions\"]).to(device)\n",
    "        for i in range(iters):\n",
    "            deltas, returns = self.get_phi(trajectory, self.v_fn)\n",
    "            phi = deltas / returns.std()\n",
    "            v_fn_loss = torch.mean((self.v_fn(states) - returns.detach()) ** 2)\n",
    "            mu_p, logsigma_p = self.beta(states)\n",
    "            sigma_p = torch.exp(logsigma_p)\n",
    "            dist_p = Normal(mu_p, sigma_p)\n",
    "            lp_p = dist_p.log_prob(actions)\n",
    "            ratio = torch.exp((lp_p - log_probs.detach()).sum(dim=-1, keepdim=True))\n",
    "            clipped_objective = torch.min(ratio * phi, torch.clamp(ratio, 1 + eps, 1 - eps) * phi)\n",
    "            pol_loss = -torch.mean(clipped_objective)\n",
    "            loss = pol_loss + v_fn_loss\n",
    "            optim.zero_grad()\n",
    "            if i < iters-1:\n",
    "                loss.backward(retain_graph=True)\n",
    "            else:\n",
    "                loss.backward()\n",
    "            optim.step()\n",
    "        return pol_loss.item(), v_fn_loss.item()\n",
    "    \n",
    "def test(env, agent):\n",
    "    state = torch.Tensor(env.reset()).to(device)\n",
    "    done = False\n",
    "    r = 0.\n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action, _, _ = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action.cpu().data.numpy())\n",
    "        r += reward\n",
    "        next_state = torch.Tensor(next_state).to(device)\n",
    "        state = next_state\n",
    "    return r\n",
    "\n",
    "\n",
    "def rollout(env, agent, batch_size):\n",
    "    s_, a_, ns_, r_, lp_, masks = [], [], [], [], [], []\n",
    "    T = 0\n",
    "    while T < batch_size:\n",
    "        t = 0\n",
    "        state = torch.Tensor(env.reset()).to(device)\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, log_prob, entropy = agent.select_action(state)\n",
    "            next_state, reward, done, info = env.step(action.cpu().data.numpy())\n",
    "            reward = torch.Tensor([reward]).to(device)\n",
    "            next_state = torch.Tensor(next_state).to(device)\n",
    "            s_.append(state)\n",
    "            a_.append(action)\n",
    "            ns_.append(next_state)\n",
    "            r_.append(reward)\n",
    "            lp_.append(log_prob)\n",
    "            masks.append(torch.Tensor([not done]).to(device))\n",
    "            state = next_state\n",
    "            t += 1\n",
    "        T += t\n",
    "    trajectory = {\n",
    "                \"states\" : s_,\n",
    "                \"actions\" : a_,\n",
    "                \"rewards\" : r_,\n",
    "                \"next_states\" : ns_,\n",
    "                \"masks\" : masks,\n",
    "                \"log_probs\" : lp_,\n",
    "                }\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "def train_offline(env, agent, opt, batch_size=1024, iterations=500, log_interval=10, t_runs=10):\n",
    "    test_rew_best = np.mean([test(env, agent) for _ in range(t_runs)])\n",
    "    data = []\n",
    "    data.append(test_rew_best)\n",
    "    print()\n",
    "    print(\"Iterations: \", 0)\n",
    "    print(\"Time steps: \", 0)\n",
    "    print(\"Reward: \", test_rew_best)\n",
    "    print()\n",
    "    for ep in range(1, iterations+1):\n",
    "        trajectory = rollout(env, agent, batch_size)\n",
    "        agent.update(opt, trajectory)\n",
    "        if ep % log_interval == 0:\n",
    "            test_rew = np.mean([test(env, agent) for _ in range(t_runs)])\n",
    "            data.append(test_rew)\n",
    "            print(\"Iterations: \", ep)\n",
    "            print(\"Time steps: \", batch_size*ep)\n",
    "            print(\"Reward: \", test_rew)\n",
    "            print()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
